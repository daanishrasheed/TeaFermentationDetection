{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Build a CNN model \n",
    "2. Train the model\n",
    "3. Assess model performance\n",
    "\n",
    "Looks like a three-step process but it will be more.\n",
    "\n",
    "## Model building\n",
    "With reference to the research paper, we will refer to this section frequently:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\daani\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\daani\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\daani\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\daani\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\daani\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\daani\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\daani\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\daani\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\daani\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\daani\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\daani\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\daani\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import models, optimizers\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, TeaNet is based on the AlexNet architecture. \n",
    "\n",
    "To make your time building TeaNet easier, why not start from AlexNet and then start modifying? \n",
    "\n",
    "Very useful resource if you don't know how to create your CNN layers from scratch: https://analyticsindiamag.com/hands-on-guide-to-implementing-alexnet-with-keras-for-multi-class-image-classification/\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 96)          34944     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 96)          384       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8, 8, 96)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 96)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 256)         614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 2, 384)         885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2, 2, 384)         1536      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 2, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 2, 384)         1536      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 2, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 256)         884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                10010     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 25,730,506\n",
      "Trainable params: 25,709,350\n",
      "Non-trainable params: 21,156\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Implement the original AlexNet\n",
    "#Instantiation\n",
    "AlexNet = models.Sequential()\n",
    "\n",
    "#1st Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#2nd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#3rd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#4th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#5th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#Passing it to a Fully Connected layer\n",
    "AlexNet.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#2nd Fully Connected Layer\n",
    "AlexNet.add(Dense(4096))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "#Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#3rd Fully Connected Layer\n",
    "AlexNet.add(Dense(1000))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "#Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#Output Layer\n",
    "AlexNet.add(Dense(10))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('softmax'))\n",
    "\n",
    "#Model Summary\n",
    "AlexNet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Modify AlexNet and turn it into TeaNet\n",
    "Now that you've successfully implemented AlexNet, time to modify the parameters according to the publication. \n",
    "\n",
    "We'll tweak it to ask close as it's needed to TeaNet (plus a bit of our own modification):\n",
    "\n",
    "<strong>More specifically, to turn AlexNet into TeaNet, you will need to:</strong>\n",
    "1. Change the input_shape parameter in first layer\n",
    "2. Change the filter sizes in each layer to match the publication\n",
    "3. Remove 'padding' parameter in all layers\n",
    "4. Change the drop out ratio\n",
    "5. Completely remove the # Output Layer\n",
    "6. In the 3rd fully connected layer:\n",
    "    - Change the number of neurons in the third fully connected layer from 1000 to 3\n",
    "    - Change the activation from 'relu' to 'softmax'\n",
    "    - Remove the dropout operation\n",
    "7. Change the compile parameters\n",
    "    - Change the optimizer from adam to optimizers.SGD with a learning rate of 0.01, and momentum of 0.9\n",
    "    \n",
    "At the model summary, see if your parameters look like this:\n",
    "```\n",
    "Total params: 239,567\n",
    "Trainable params: 237,833\n",
    "Non-trainable params: 1,734\n",
    "```\n",
    "\n",
    "You're on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 35, 35, 32)        11648     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 35, 35, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 35, 35, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 17, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 2, 2, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 239,567\n",
      "Trainable params: 237,833\n",
      "Non-trainable params: 1,734\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Modify AlexNet to TeaNet\n",
    "#Instantiation\n",
    "Alex = models.Sequential()\n",
    "\n",
    "#1st Convolutional Layer\n",
    "Alex.add(Conv2D(filters=32, input_shape=(150,150,3), kernel_size=(11,11), strides=(4,4)))\n",
    "Alex.add(BatchNormalization())\n",
    "Alex.add(Activation('relu'))\n",
    "Alex.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "\n",
    "#2nd Convolutional Layer\n",
    "Alex.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1,1)))\n",
    "Alex.add(BatchNormalization())\n",
    "Alex.add(Activation('relu'))\n",
    "Alex.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "#3rd Convolutional Layer\n",
    "Alex.add(Conv2D(filters=128, kernel_size=(3,3), strides=(3,3)))\n",
    "Alex.add(BatchNormalization())\n",
    "Alex.add(Activation('relu'))\n",
    "Alex.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "#Passing it to a Fully Connected layer\n",
    "Alex.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "Alex.add(Dense(512, input_shape=(150,150,3,)))\n",
    "Alex.add(BatchNormalization())\n",
    "Alex.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "Alex.add(Dropout(0.5))\n",
    "\n",
    "#2nd Fully Connected Layer\n",
    "Alex.add(Dense(128))\n",
    "Alex.add(BatchNormalization())\n",
    "Alex.add(Activation('relu'))\n",
    "#Add Dropout\n",
    "Alex.add(Dropout(0.5))\n",
    "\n",
    "#3rd Fully Connected Layer\n",
    "Alex.add(Dense(3))\n",
    "Alex.add(BatchNormalization())\n",
    "Alex.add(Activation('softmax'))\n",
    "\n",
    "#Model Summary\n",
    "Alex.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "Alex.compile(loss = keras.losses.categorical_crossentropy, optimizer= optimizers.SGD(lr=0.001, momentum=0.9), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your images\n",
    "We haven't set our training, validation, and test data yet so we can't train. \n",
    "\n",
    "### Step 4: Define the classes\n",
    "Declare a variable, and assign it with a dictionary with the following key-value pair:\n",
    "1. 'underfermented tea' - 0\n",
    "2. 'overfermented tea' - 1\n",
    "3. 'fermented tea' - 2\n",
    "\n",
    "The reason why we do this is that sometimes model training on Keras can lead the classes being mixed up. As such, it's better to explicitly declare what our classes and its corresponding values are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'underfermented tea': 0, 'overfermented tea': 1, 'fermented tea': 2}\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Declare your classes\n",
    "d = {'underfermented tea':0, 'overfermented tea':1, 'fermented tea':2}\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create ImageDataGenerator objects\n",
    "We will be generating batches of augmented image data during model training, so we will be creating ImageDataGenerator objects. \n",
    "\n",
    "Handy reading: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "\n",
    "Declare three variables, and assign them ImageDataGenerator objects with the following parameters\n",
    "1. train generator\n",
    "    - rescale - 1.0/255\n",
    "    - shear_range - 0.2\n",
    "    - zoom_range - 0.2\n",
    "    - horizontal_flip - True\n",
    "2. test generator\n",
    "    - rescale - 1.0/255\n",
    "3. val generator\n",
    "    - rescale - 1.0/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingen = ImageDataGenerator(shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True, rescale = 1.0/255)\n",
    "testgen = ImageDataGenerator(rescale = 1.0/255)\n",
    "valgen = ImageDataGenerator(rescale = 1.0/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Declare your train/val/test folder paths\n",
    "Declare three variables, and assign them the path to your train, val, and test folders. \n",
    "\n",
    "Navigate to the folders that you got from Part II Step 6, and copy the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Store the path to variables\n",
    "train_path = 'Black tea fermentation dataset/output/train'\n",
    "val_path = 'Black tea fermentation dataset/output/val'\n",
    "test_path = 'Black tea fermentation dataset/output/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Declare your train/validation/test .flow_from_directory\n",
    "Next up, we will declare three new variables, and in each variable you use the .flow_from_directory method from your ImageDataGenerators that you created in Step 5. \n",
    "\n",
    "Here are the parameters to use:\n",
    "1. train set\n",
    "    - path - the path you declared from Step 6\n",
    "    - target_size - (150, 150)\n",
    "    - batch_size - 16\n",
    "    - class_mode - 'categorical'\n",
    "    - classes - the dictonary from Step 4\n",
    "2. test set\n",
    "    - same as train generator\n",
    "    - shuffle - False\n",
    "3. val set\n",
    "    - same as train generator\n",
    "    - shuffle - False\n",
    "\n",
    "If your folder structure is correct, split your files properly, and declare the right paths, you'll see this:\n",
    "\n",
    "![FlowFromDirectoryResults](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectTeaFermentation/FlowFromDirectoryOutput.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4800 images belonging to 3 classes.\n",
      "Found 600 images belonging to 3 classes.\n",
      "Found 601 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Declare your .flow_from_directory variables\n",
    "train = traingen.flow_from_directory(directory=train_path, target_size=(150,150), classes=d, class_mode='categorical', batch_size=16)\n",
    "val = valgen.flow_from_directory(directory=val_path, target_size=(150,150), classes=d, class_mode='categorical', batch_size=16, shuffle=False)\n",
    "test = testgen.flow_from_directory(directory=test_path, target_size=(150,150), classes=d, class_mode='categorical', batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and assessment\n",
    "### Step 8: Train your CNN\n",
    "Now that everything is set up, we can now train our model. \n",
    "\n",
    "Declare a variable called history, and assign it to your model's .fit() method with the following parameters:\n",
    "1. train_set - train set from Step 7\n",
    "2. steps_per_epoch - 20\n",
    "3. epochs - 20\n",
    "4. verbose - 1\n",
    "5. validation_set - val set from Step 7\n",
    "\n",
    "If everything works, you'll see something like this:\n",
    "\n",
    "![TrainOutput](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectTeaFermentation/TrainOutput.png)\n",
    "\n",
    "Be patient and revisit Steps 2-3 if it doesn't work. You should have\n",
    "1. low acc_loss\n",
    "2. low val_loss\n",
    "3. high acc\n",
    "4. high val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 8s 416ms/step - loss: 0.9354 - accuracy: 0.5875 - val_loss: 1.0011 - val_accuracy: 0.3333\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 5s 274ms/step - loss: 0.4312 - accuracy: 0.9531 - val_loss: 0.9712 - val_accuracy: 0.3333\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 5s 269ms/step - loss: 0.3370 - accuracy: 0.9937 - val_loss: 0.9914 - val_accuracy: 0.6683\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 6s 275ms/step - loss: 0.3089 - accuracy: 0.9969 - val_loss: 0.9818 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 6s 292ms/step - loss: 0.3235 - accuracy: 0.9906 - val_loss: 1.0004 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 5s 270ms/step - loss: 0.2948 - accuracy: 0.9937 - val_loss: 0.9987 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 5s 265ms/step - loss: 0.2805 - accuracy: 1.0000 - val_loss: 0.9680 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 5s 268ms/step - loss: 0.2778 - accuracy: 0.9969 - val_loss: 0.8985 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 5s 270ms/step - loss: 0.2637 - accuracy: 0.9969 - val_loss: 0.9189 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 5s 257ms/step - loss: 0.2342 - accuracy: 1.0000 - val_loss: 0.9441 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 6s 276ms/step - loss: 0.2454 - accuracy: 0.9969 - val_loss: 0.9210 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 5s 268ms/step - loss: 0.2416 - accuracy: 1.0000 - val_loss: 0.8145 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 5s 268ms/step - loss: 0.2364 - accuracy: 1.0000 - val_loss: 0.8118 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 6s 281ms/step - loss: 0.2264 - accuracy: 0.9969 - val_loss: 0.8057 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 6s 276ms/step - loss: 0.2275 - accuracy: 1.0000 - val_loss: 0.8290 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 6s 297ms/step - loss: 0.2261 - accuracy: 1.0000 - val_loss: 0.7240 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 6s 281ms/step - loss: 0.1949 - accuracy: 1.0000 - val_loss: 0.6996 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 6s 277ms/step - loss: 0.1902 - accuracy: 1.0000 - val_loss: 0.6210 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 5s 257ms/step - loss: 0.2083 - accuracy: 0.9937 - val_loss: 0.6125 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 5s 266ms/step - loss: 0.2001 - accuracy: 1.0000 - val_loss: 0.5126 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Train your model\n",
    "history = Alex.fit(train, steps_per_epoch=20, epochs=20, verbose=1, validation_data=val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Make predictions using test set\n",
    "After we're done training, you will notice that both accuracy and validation accuracy is high. How does it fare against the test set? \n",
    "\n",
    "Use the .predict method of your CNN model and use the test set variable that you declared in Step 7 to make your predictions.\n",
    "\n",
    "You should see something like this: \n",
    "\n",
    "![TestSetPrediction](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectTeaFermentation/PredictionFromTestSet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Make predictions using your test set\n",
    "x = Alex.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7225481 , 0.14641587, 0.13103595],\n",
       "       [0.7225481 , 0.14641587, 0.13103595],\n",
       "       [0.7225481 , 0.14641587, 0.13103595],\n",
       "       ...,\n",
       "       [0.1645663 , 0.24178176, 0.59365195],\n",
       "       [0.1645663 , 0.24178176, 0.59365195],\n",
       "       [0.1645663 , 0.24178176, 0.59365195]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Get the index of maximum value in each prediction array\n",
    "If you've noticed, each item in the list of prediction is another list containing the probabilities of each class. \n",
    "\n",
    "Use numpy's argmax method to get the index of the highest value in the array - that is your class prediction.\n",
    "\n",
    "<strong>Hint: Google \"How to find the predicted class for a specific testing sample\"</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Assess your prediction with a confusion matrix\n",
    "Earlier on, you imported confusion_matrix from sklearn.metrics. \n",
    "\n",
    "Now's the time to use it, as you get the .classes attribute from your test set from Step 7, and compare it with the predictions you got from Step 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Print a confusion matrix\n",
    "y_true = test.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=['underfermented', 'overfermented', 'fermented']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mtx=confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[200,   0,   0],\n",
       "       [  0, 200,   0],\n",
       "       [  0,   0, 201]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Plot your accuracy and loss\n",
    "Wait a minute - the results from Step 11 seems too good. What's wrong, and are we overfitting?\n",
    "\n",
    "Judging from the fact that we used a test set that was completely separate from train set and validation set, we can say that our results are great. \n",
    "\n",
    "Nonetheless, let's plot graphs:\n",
    "1. accuracy vs epoch\n",
    "2. loss vs epoch\n",
    "\n",
    "And in each graph we compare the changes over time for both train and val over the epochs.\n",
    "\n",
    "To retrieve the information you need, you can use the .history attribute from the 'history' variable that you declared in Step 8.\n",
    "\n",
    "![TrainValAccuracyLoss](https://uplevelsg.s3-ap-southeast-1.amazonaws.com/ProjectTeaFermentation/TrainValAccuracyLossPlot.png)\n",
    "\n",
    "You'll see something like this.\n",
    "\n",
    "<strong>Hint: Google \"keras plot training, validation and test set accuracy\"</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Plot your accuracy and val plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Part III\n",
    "That was long, wasn't it? But based on the results from Step 11 and Step 12, we would say it's well worth it. \n",
    "\n",
    "If you carried everything out well so far, congratulations! \n",
    "\n",
    "You're theoretically supposed to train a model that can predict 100% the state of fermentation of the leave based on image data alone. \n",
    "\n",
    "If you haven't, it's a good time to go back and revisit steps that you might have executed wrongly. \n",
    "\n",
    "Otherwise, let's head on to Parts IV and V where we will take a step back and return to our machine learning roots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
